# Module 4: CLI workflows for newsrooms

## Discussion forum prompts

---

## Discussion 1: Repetitive tasks in your newsroom

Think about your current workflow. What tasks do you do repeatedlyâ€”daily, weekly, or for every story?

**Prompt:**

Describe one repetitive task in your newsroom that you think could be automated with a CLI pipeline. Be specific:

- What are the inputs? (URLs, documents, data sources)
- What processing happens? (summarizing, formatting, extracting)
- What's the output? (email, spreadsheet, published content)

Don't worry if you don't know how to build it yet. The goal is to identify the opportunity.

Reply to at least one classmate with a suggestion for how their task might be approached.

---

## Discussion 2: When automation goes wrong

Automation can fail. Scrapers break when websites change. APIs get rate-limited. AI outputs need human review.

**Prompt:**

What safeguards would you build into an automated pipeline for journalism work? Consider:

- How would you handle errors?
- When should a human review the output before it goes anywhere?
- What would you never fully automate?

Share your thinking and respond to one classmate whose approach differs from yours.

---

## Discussion 3: Local vs. cloud AI for newsroom tools

This week we covered both cloud AI (Claude's API) and local AI (Ollama). Each has tradeoffs:

- **Cloud AI:** More capable, no local hardware needed, but sends data to external servers
- **Local AI:** Data stays on your machine, works offline, but requires setup and may be less capable

**Prompt:**

For your newsroom's use case, which would you prefer? What factors influence your decision?

Consider: data sensitivity, internet reliability, cost, technical capacity, and specific use cases.
